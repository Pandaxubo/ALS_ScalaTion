import scalation.linalgebra.MatrixD
import scalation.linalgebra.VectorD
import scalation.linalgebra.MatrixI
import scalation.random.RandomMatD
import scala.util.Random
import scala.math.abs
import scala.io.Source.fromFile

import MatrixD.eye

//::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
/** The `ALSReg` class is used to predict the missing values of an input matrix
 *  by applying Alternating Least Square to factor the matrix. Compare to ALS Explicit
 *  method(ALSFast), it will be more accurate but slower.
 *  Once the factors are obtained the missing value in the matrix is obtained as the
 *  dot product of 'x.t' and 'y', where
 *  <p>
 *      x is the user-factors vector (nf * nu)
 *      y is the item-factors vector (nf * ni)
 *      predict (i, j) = x.t dot y
 *  <p>
 *------------------------------------------------------------------------------
 *  @param a  the input data matrix
 */
class ALSReg(a: MatrixD){

  var r_lambda = 0.1 //normalization parameter
  var nf = 20  //dimension of latent vector of each user and item
  val ni = a.dim2 //number of items
  val nu = a.dim1 //number of users
  var n_epochs = 16   //Number of epochs
  var E = eye(nf) //(nf x nf)-dimensional identity matrix

  //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
  /**  Optimization Function for user.
    *  @param X  the user-factors vector
    *  @param Y  the item-factors vector
    *  @param E  the compressed matrix(nf * nf)
    *  @param I  "selector" matrix I
    *  @param r_lambda  the normalization parameter
    */
  def optimize_user(X: MatrixD, Y: MatrixD, r_lambda:Double, E: MatrixD, I: MatrixD)={
    for (i <- X.range2) {
      print(s"\r user: ${i+1} / "+ X.dim2)

      var nui = I(i).countPos
      var Ii_nonzero = new VectorD(1)
      for(j <- I.range2){
          if(I(i)(j) != 0){
              if (j == 0){
                Ii_nonzero(0) = j
              }
              else{
                Ii_nonzero = Ii_nonzero ++ j
              }
          }
      }
      if(nui == 0)  nui = 1

      var Y_Ii = new MatrixD(Y.dim1, nui)
      for(k <- 0 until nui){
        Y_Ii.setCol(k, Y.col(Ii_nonzero(k).toInt))
      }

      var a_Ii = new MatrixD(1, nui)
      for(j <- 0 until nui){
        a_Ii(0,j) =  a(i)(Ii_nonzero(j).toInt)
      }

      var Ai = (Y_Ii.t).mdot(Y_Ii.t) + E * r_lambda * nui
      var Vi = ((Y_Ii.t).mdot(a_Ii.t)).col(0)

      X.setCol(i, (Ai).solve(Vi))

    }
    println()
  }

  //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
  /**  Optimization Function for item.
    *  @param X  the user-factors vector
    *  @param Y  the item-factors vector
    *  @param E  the compressed matrix(nf * nf)
    *  @param I  "selector" matrix I
    *  @param r_lambda  the normalization parameter
    */
  def optimize_item(X: MatrixD, Y: MatrixD, r_lambda:Double, E: MatrixD, I: MatrixD) = {
    for (j <- Y.range2) {
      print(s"\r item: ${j+1} / "+ Y.dim2)

      var nmj = I(j).countPos
      var Ij_nonzero = new VectorD(1)
      for(i <- I.range2){
          if(I(j)(i) != 0){
            if (i == 0){
              Ij_nonzero(0) = i
              }
            else{
              Ij_nonzero = Ij_nonzero ++ i
            }
          }
      }
      if(nmj == 0)  nmj = 1

      var X_Ij = new MatrixD(X.dim1, nmj)
      for(k <- 0 until nmj){
        X_Ij.setCol(k, X.col(Ij_nonzero(k).toInt))
      }

      var a_Ij = new MatrixD(nmj, 1)
      for(i <- 0 until nmj){
        a_Ij(i,0) =  a(Ij_nonzero(i).toInt)(j)
      }

      var Aj = (X_Ij.t).mdot(X_Ij.t) + E * r_lambda * nmj
      var Vj = ((X_Ij.t).mdot(a_Ij)).col(0)

      Y.setCol(j, (Aj).solve(Vj))
    }
    println()
  }

  //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
  /**  Get the average value of the input dataset for normalization
    *  @param file_name  the input dataset's path
    */
  def getAverage(file_name: String): Double = {
    val tar = MatrixI(file_name)
    tar.col(2).sum / tar.dim1.toDouble
  }


  //::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
  /** Train the model to get predict matrix using input dataset.
    *  @see Large-scale Parallel Collaborative Filtering for the Netflix Prize(Section 3.1)
    *  @see http://shiftleft.com/mirrors/www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf
    */
  def train(I: MatrixD, I2: MatrixD): MatrixD = {
    val target = new ALSReg(a)
    println("Start algorithm.")
    println("User: "+I.dim1+","+"Item: "+I.dim2)
    var X = RandomMatD (target.nf, target.nu, 1, 0, 1, 0).gen * 3
    var Y = RandomMatD (target.nf, target.ni, 1, 0, 1, 0).gen * 3

    var Y_vector: VectorD = a(0)
    for(i <- 1 until a.dim1){
        Y_vector = Y_vector ++ a(i)
    }

    var avgRating = Y_vector.sum/(Y_vector.countPos)
    for(i <- Y.range2){
      Y(0, i) = avgRating
    }


    for (i <- 0 until n_epochs) {
      println("----------------step "+i+"----------------")
      if (i != 0){
          target.optimize_user(X, Y, target.r_lambda, target.E, I)
          target.optimize_item(X, Y, target.r_lambda, target.E, I.t)
      }//compute X and Y iteratively
    }
    var predict = (X).mdot(Y) //final predict matrix

    println("----------------End----------------")
    predict
  }
}






